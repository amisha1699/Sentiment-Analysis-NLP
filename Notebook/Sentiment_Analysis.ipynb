{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "pJXT6oh9MIVY",
        "outputId": "274bc0bd-a984-4dd3-ae05-f186e28fe108"
      },
      "outputs": [],
      "source": [
        "!pip install praw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vgY5CnPoLAFo"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import praw\n",
        "import nltk\n",
        "import os\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from textblob import TextBlob\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1Qo_QJ1RMBCx"
      },
      "outputs": [],
      "source": [
        "def preprocessPosts(original_post):\n",
        "    # Load the English stopwords in NLTK\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # Remove the stop words from the post\n",
        "    post = \" \".join([word for word in original_post.split() if word.lower() not in stop_words])\n",
        "\n",
        "    # Remove URLs\n",
        "    post = re.sub(r\"http\\S+\", \"\", post)\n",
        "\n",
        "    # Remove emojis and other unwanted characters\n",
        "    post = re.sub(r'[^\\w\\s#@/:%.,_-]', '', post)\n",
        "    return post"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GVkqqsfUMQPk"
      },
      "outputs": [],
      "source": [
        "def findPosts(topic):\n",
        "    # Set up your Reddit API credentials\n",
        "    client_id = \"REPALCE_WITH_YOUR_CLIENT_ID\"\n",
        "    client_secret = \"REPALCE_WITH_YOUR_CLIENT_SECRET\"\n",
        "    user_agent = \"REPALCE_WITH_YOUR_USER_ID\"\n",
        "\n",
        "    # Authenticate with the Reddit API using PRAW\n",
        "    reddit = praw.Reddit(client_id=client_id, client_secret=client_secret, user_agent=user_agent)\n",
        "\n",
        "    # Grab 100 posts with the requested topic\n",
        "    posts = []\n",
        "    for submission in reddit.subreddit(\"all\").search(topic, limit=100):\n",
        "        posts.append(submission.title + \" \" + submission.selftext)\n",
        "    print(\"Collected %d posts about %s.\" % (len(posts), topic))\n",
        "    return posts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "oFXBWIS6N4-L"
      },
      "outputs": [],
      "source": [
        "def sentimentAnalysis(text):\n",
        "    return TextBlob(text).sentiment.polarity\n",
        "\n",
        "def plotSentimentScores(sentiments):\n",
        "    plt.scatter(range(len(sentiments)), sentiments, c=['green' if s >= 0 else 'red' for s in sentiments], alpha=0.5)\n",
        "    plt.title('Sentiment Score Distribution')\n",
        "    plt.xlabel('Sentiment Score')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.axhline(0, color='grey', linestyle='--', linewidth=0.7)\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "def labelSentiment(score):\n",
        "    return 'Positive' if score >= 0 else 'Negative'\n",
        "\n",
        "def plotWordCloud(text):\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOrrXanONQFs",
        "outputId": "24c96014-9e82-4692-b64f-318e8648a74c"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Download the stopwords if necessary\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "    # Search for posts about the chosen topic\n",
        "    posts = findPosts(\"Computational Biology\")\n",
        "\n",
        "    # Define the sentiment analyzer\n",
        "    sentimentAnalysis = lambda text: TextBlob(text).sentiment.polarity\n",
        "\n",
        "    # Open a CSV file to store the retrieved posts and sentiment scores\n",
        "    with open('posts_sentiment.csv', mode='w', newline='', encoding=\"utf-8\") as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Post', 'Sentiment Score'])\n",
        "\n",
        "        sentiments = []\n",
        "        all_text = \"\"\n",
        "        print(\"Performing Sentiment Analysis on the posts.\")\n",
        "        # Analyze the sentiment of each post and write it to the CSV file\n",
        "        for post in posts:\n",
        "            clean_post = preprocessPosts(post)\n",
        "\n",
        "            # Analyze the sentiment of the filtered post\n",
        "            sentiment = sentimentAnalysis(clean_post)\n",
        "            sentiments.append(sentiment)\n",
        "\n",
        "            # Label the sentiment\n",
        "            label = labelSentiment(sentiment)\n",
        "\n",
        "            # Aggregate text for word cloud\n",
        "            all_text += \" \" + clean_post\n",
        "\n",
        "            # Write the post, sentiment score, and label to the CSV file\n",
        "            writer.writerow([clean_post, sentiment, label])\n",
        "\n",
        "    print(\"CSV file with posts and sentiment scores saved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjyZ--gSNw2Z",
        "outputId": "88379cb2-d9c8-4c7f-f62b-9790b6dee2c2"
      },
      "outputs": [],
      "source": [
        "posts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "N7zK3QOPOFCH",
        "outputId": "60640cc8-f3e2-42e2-cd8c-bc08dadd998f"
      },
      "outputs": [],
      "source": [
        "plotSentimentScores(sentiments)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "3AdQ9YmjOiEl",
        "outputId": "10d6ab6f-5e55-49fb-c7bf-2128d930a984"
      },
      "outputs": [],
      "source": [
        "plotWordCloud(all_text)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
